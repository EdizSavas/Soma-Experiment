import requests
from sentence_transformers import SentenceTransformer
import numpy as np
import json

class SomaFree:
    """
    %100 Ãœcretsiz SOMA - Ollama kullanÄ±yor
    """
    def __init__(self, ollama_url="http://localhost:11434"):
        print("ğŸ§  SOMA (Ãœcretsiz Versiyon) baÅŸlatÄ±lÄ±yor...")
        
        # Embedding modeli (Ã¼cretsiz, yerelde Ã§alÄ±ÅŸÄ±r)
        print("ğŸ“¥ Embedding modeli yÃ¼kleniyor...")
        self.embedding_model = SentenceTransformer(
            'emrecan/bert-base-turkish-cased-mean-nli-stsb-tr'
        )
        
        # Ollama baÄŸlantÄ±sÄ±
        self.ollama_url = ollama_url
        self.model_name = "llama3.1:8b"  # KullanÄ±lacak model
        
        # Ollama'nÄ±n Ã§alÄ±ÅŸÄ±p Ã§alÄ±ÅŸmadÄ±ÄŸÄ±nÄ± kontrol et
        self._check_ollama()
        
        print("âœ… SOMA hazÄ±r!")
    
    def _check_ollama(self):
        """Ollama'nÄ±n Ã§alÄ±ÅŸÄ±p Ã§alÄ±ÅŸmadÄ±ÄŸÄ±nÄ± kontrol et"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags")
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [m['name'] for m in models]
                
                if self.model_name not in model_names:
                    print(f"âš ï¸  {self.model_name} modeli bulunamadÄ±!")
                    print(f"   Terminalde ÅŸunu Ã§alÄ±ÅŸtÄ±r: ollama pull {self.model_name}")
                    raise Exception(f"Model yok: {self.model_name}")
                else:
                    print(f"âœ… Ollama baÄŸlantÄ±sÄ± baÅŸarÄ±lÄ± ({self.model_name})")
        except requests.exceptions.ConnectionError:
            print("âŒ Ollama'ya baÄŸlanÄ±lamÄ±yor!")
            print("   Terminalde ÅŸunu Ã§alÄ±ÅŸtÄ±r: ollama serve")
            raise
    
    def anlamsal_arama(self, sorgu, kayitlar, top_k=3):
        """
        Anlamsal arama yapan ana fonksiyon
        """
        print(f"\nğŸ” Aranan: '{sorgu}'")
        
        # 1. Sorguyu vektÃ¶re Ã§evir
        sorgu_vektoru = self.embedding_model.encode(sorgu)
        
        # 2. TÃ¼m kayÄ±tlarÄ± vektÃ¶re Ã§evir
        kayit_metinleri = [k["notlar"] for k in kayitlar]
        kayit_vektorleri = self.embedding_model.encode(kayit_metinleri)
        
        # 3. Benzerlik hesapla (cosine similarity)
        benzerlikler = []
        for i, kayit_vektoru in enumerate(kayit_vektorleri):
            benzerlik = np.dot(sorgu_vektoru, kayit_vektoru) / (
                np.linalg.norm(sorgu_vektoru) * np.linalg.norm(kayit_vektoru)
            )
            benzerlikler.append({
                "kayit": kayitlar[i],
                "skor": float(benzerlik)
            })
        
        # 4. Skora gÃ¶re sÄ±rala
        benzerlikler.sort(key=lambda x: x["skor"], reverse=True)
        
        print(f"\nğŸ“Š En yakÄ±n {top_k} sonuÃ§:")
        for i, sonuc in enumerate(benzerlikler[:top_k], 1):
            print(f"{i}. {sonuc['kayit']['isim']} (Skor: {sonuc['skor']:.3f})")
            print(f"   Not: {sonuc['kayit']['notlar'][:60]}...")
        
        return benzerlikler[:top_k]
    
    def llm_ile_cevapla(self, sorgu, bulunan_kayitlar):
        """
        Ollama ile doÄŸal dil yanÄ±tÄ± Ã¼ret
        """
        # KayÄ±tlarÄ± prompt iÃ§in hazÄ±rla
        kayit_metni = "\n\n".join([
            f"Hasta {i+1}: {k['kayit']['isim']}\n"
            f"Tarih: {k['kayit']['tarih']}\n"
            f"Notlar: {k['kayit']['notlar']}\n"
            f"Benzerlik Skoru: {k['skor']:.2f}"
            for i, k in enumerate(bulunan_kayitlar)
        ])
        
        prompt = f"""Sen bir terapist asistanÄ±sÄ±n. Hasta kayÄ±tlarÄ±nda arama yaptÄ±n ve sonuÃ§larÄ± kullanÄ±cÄ±ya aÃ§Ä±klaman gerekiyor.

KULLANICI SORUSU: {sorgu}

BULUNAN KAYITLAR:
{kayit_metni}

LÃ¼tfen kullanÄ±cÄ±ya, bulduÄŸun en alakalÄ± hasta/hastalarÄ± kÄ±sa ve Ã¶z bir ÅŸekilde aÃ§Ä±kla. Hasta gizliliÄŸine dikkat et ama yardÄ±mcÄ± ol. TÃ¼rkÃ§e yanÄ±t ver."""

        print("\nğŸ¤– Ollama cevap hazÄ±rlÄ±yor...")
        
        try:
            # Ollama API'sine istek gÃ¶nder
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "num_predict": 256  # Maksimum token sayÄ±sÄ±
                    }
                },
                timeout=60  # 60 saniye timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                return result['response'].strip()
            else:
                return f"âŒ Ollama hatasÄ±: {response.status_code}"
                
        except requests.exceptions.Timeout:
            return "â±ï¸ Zaman aÅŸÄ±mÄ± - Ollama yanÄ±t vermedi (60 saniye)"
        except Exception as e:
            return f"âŒ Hata: {str(e)}"
    
    def llm_ile_cevapla_streaming(self, sorgu, bulunan_kayitlar):
        """
        Stream modunda cevap (kelime kelime yazÄ±yor gibi)
        """
        kayit_metni = "\n\n".join([
            f"Hasta {i+1}: {k['kayit']['isim']}\n"
            f"Tarih: {k['kayit']['tarih']}\n"
            f"Notlar: {k['kayit']['notlar']}\n"
            f"Benzerlik Skoru: {k['skor']:.2f}"
            for i, k in enumerate(bulunan_kayitlar)
        ])
        
        prompt = f"""Sen bir terapist asistanÄ±sÄ±n. Hasta kayÄ±tlarÄ±nda arama yaptÄ±n.

KULLANICI SORUSU: {sorgu}

BULUNAN KAYITLAR:
{kayit_metni}

KÄ±sa ve Ã¶z aÃ§Ä±kla. TÃ¼rkÃ§e yanÄ±t ver."""

        print("\nğŸ¤– Ollama cevap hazÄ±rlÄ±yor (stream)...")
        
        try:
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": True,
                    "options": {"temperature": 0.7}
                },
                stream=True,
                timeout=60
            )
            
            full_response = ""
            for line in response.iter_lines():
                if line:
                    chunk = json.loads(line)
                    if 'response' in chunk:
                        token = chunk['response']
                        print(token, end='', flush=True)
                        full_response += token
            
            print()  # Yeni satÄ±r
            return full_response.strip()
            
        except Exception as e:
            return f"âŒ Hata: {str(e)}"